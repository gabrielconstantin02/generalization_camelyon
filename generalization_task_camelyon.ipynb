{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from wilds import get_dataset\n",
    "from wilds.common.data_loaders import get_train_loader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from wilds.common.data_loaders import get_eval_loader\n",
    "import torch\n",
    "from torch import nn\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load the full dataset, and download it if necessary\n",
    "dataset = get_dataset(dataset=\"camelyon17\", download=True)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "'\\n# Train loop\\nfor labeled_batch, unlabeled_batch in zip(train_loader, unlabeled_loader):\\n    x, y, metadata = labeled_batch\\n    unlabeled_x, unlabeled_metadata = unlabeled_batch\\n    ...\\n'"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Get the training set\n",
    "train_data = dataset.get_subset(\n",
    "    \"train\",\n",
    "    transform=transforms.Compose(\n",
    "        [transforms.Resize((224, 224)), transforms.ToTensor()]\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Prepare the standard data loader\n",
    "train_loader = get_train_loader(\"standard\", train_data, batch_size=16)\n",
    "\n",
    "\"\"\"\n",
    "# (Optional) Load unlabeled data\n",
    "dataset = get_dataset(dataset=\"camelyon17\", download=True, unlabeled=True)\n",
    "unlabeled_data = dataset.get_subset(\n",
    "    \"test_unlabeled\",\n",
    "    transform=transforms.Compose(\n",
    "        [transforms.Resize((448, 448)), transforms.ToTensor()]\n",
    "    ),\n",
    ")\n",
    "unlabeled_loader = get_train_loader(\"standard\", unlabeled_data, batch_size=16)\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "# Train loop\n",
    "for labeled_batch, unlabeled_batch in zip(train_loader, unlabeled_loader):\n",
    "    x, y, metadata = labeled_batch\n",
    "    unlabeled_x, unlabeled_metadata = unlabeled_batch\n",
    "    ...\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Get the test set\n",
    "id_val_data = dataset.get_subset(\n",
    "    \"id_val\",\n",
    "    transform=transforms.Compose(\n",
    "        [transforms.Resize((224, 224)), transforms.ToTensor()]\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Prepare the evaluation data loader\n",
    "id_val_loader = get_eval_loader(\"standard\", id_val_data, batch_size=16)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<wilds.datasets.wilds_dataset.WILDSSubset object at 0x7f028aa34f10>\n"
     ]
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Get predictions for the full test set\n",
    "for x, y_true, metadata in test_loader:\n",
    "    y_pred = model(x)\n",
    "    # Accumulate y_true, y_pred, metadata\n",
    "\n",
    "# Evaluate\n",
    "dataset.eval(all_y_pred, all_y_true, all_metadata)\n",
    "# {'recall_macro_all': 0.66, ...}\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's parameters: \n",
      "\t conv1.weight :  torch.Size([64, 3, 7, 7])\n",
      "\t bn1.weight :  torch.Size([64])\n",
      "\t bn1.bias :  torch.Size([64])\n",
      "\t layer1.0.conv1.weight :  torch.Size([64, 64, 3, 3])\n",
      "\t layer1.0.bn1.weight :  torch.Size([64])\n",
      "\t layer1.0.bn1.bias :  torch.Size([64])\n",
      "\t layer1.0.conv2.weight :  torch.Size([64, 64, 3, 3])\n",
      "\t layer1.0.bn2.weight :  torch.Size([64])\n",
      "\t layer1.0.bn2.bias :  torch.Size([64])\n",
      "\t layer1.1.conv1.weight :  torch.Size([64, 64, 3, 3])\n",
      "\t layer1.1.bn1.weight :  torch.Size([64])\n",
      "\t layer1.1.bn1.bias :  torch.Size([64])\n",
      "\t layer1.1.conv2.weight :  torch.Size([64, 64, 3, 3])\n",
      "\t layer1.1.bn2.weight :  torch.Size([64])\n",
      "\t layer1.1.bn2.bias :  torch.Size([64])\n",
      "\t layer2.0.conv1.weight :  torch.Size([128, 64, 3, 3])\n",
      "\t layer2.0.bn1.weight :  torch.Size([128])\n",
      "\t layer2.0.bn1.bias :  torch.Size([128])\n",
      "\t layer2.0.conv2.weight :  torch.Size([128, 128, 3, 3])\n",
      "\t layer2.0.bn2.weight :  torch.Size([128])\n",
      "\t layer2.0.bn2.bias :  torch.Size([128])\n",
      "\t layer2.0.downsample.0.weight :  torch.Size([128, 64, 1, 1])\n",
      "\t layer2.0.downsample.1.weight :  torch.Size([128])\n",
      "\t layer2.0.downsample.1.bias :  torch.Size([128])\n",
      "\t layer2.1.conv1.weight :  torch.Size([128, 128, 3, 3])\n",
      "\t layer2.1.bn1.weight :  torch.Size([128])\n",
      "\t layer2.1.bn1.bias :  torch.Size([128])\n",
      "\t layer2.1.conv2.weight :  torch.Size([128, 128, 3, 3])\n",
      "\t layer2.1.bn2.weight :  torch.Size([128])\n",
      "\t layer2.1.bn2.bias :  torch.Size([128])\n",
      "\t layer3.0.conv1.weight :  torch.Size([256, 128, 3, 3])\n",
      "\t layer3.0.bn1.weight :  torch.Size([256])\n",
      "\t layer3.0.bn1.bias :  torch.Size([256])\n",
      "\t layer3.0.conv2.weight :  torch.Size([256, 256, 3, 3])\n",
      "\t layer3.0.bn2.weight :  torch.Size([256])\n",
      "\t layer3.0.bn2.bias :  torch.Size([256])\n",
      "\t layer3.0.downsample.0.weight :  torch.Size([256, 128, 1, 1])\n",
      "\t layer3.0.downsample.1.weight :  torch.Size([256])\n",
      "\t layer3.0.downsample.1.bias :  torch.Size([256])\n",
      "\t layer3.1.conv1.weight :  torch.Size([256, 256, 3, 3])\n",
      "\t layer3.1.bn1.weight :  torch.Size([256])\n",
      "\t layer3.1.bn1.bias :  torch.Size([256])\n",
      "\t layer3.1.conv2.weight :  torch.Size([256, 256, 3, 3])\n",
      "\t layer3.1.bn2.weight :  torch.Size([256])\n",
      "\t layer3.1.bn2.bias :  torch.Size([256])\n",
      "\t layer4.0.conv1.weight :  torch.Size([512, 256, 3, 3])\n",
      "\t layer4.0.bn1.weight :  torch.Size([512])\n",
      "\t layer4.0.bn1.bias :  torch.Size([512])\n",
      "\t layer4.0.conv2.weight :  torch.Size([512, 512, 3, 3])\n",
      "\t layer4.0.bn2.weight :  torch.Size([512])\n",
      "\t layer4.0.bn2.bias :  torch.Size([512])\n",
      "\t layer4.0.downsample.0.weight :  torch.Size([512, 256, 1, 1])\n",
      "\t layer4.0.downsample.1.weight :  torch.Size([512])\n",
      "\t layer4.0.downsample.1.bias :  torch.Size([512])\n",
      "\t layer4.1.conv1.weight :  torch.Size([512, 512, 3, 3])\n",
      "\t layer4.1.bn1.weight :  torch.Size([512])\n",
      "\t layer4.1.bn1.bias :  torch.Size([512])\n",
      "\t layer4.1.conv2.weight :  torch.Size([512, 512, 3, 3])\n",
      "\t layer4.1.bn2.weight :  torch.Size([512])\n",
      "\t layer4.1.bn2.bias :  torch.Size([512])\n",
      "\t fc.weight :  torch.Size([1000, 512])\n",
      "\t fc.bias :  torch.Size([1000])\n",
      "Number of model parameters:  11689512\n"
     ]
    },
    {
     "data": {
      "text/plain": "'\\nyou can use this formula [(W−K+2P)/S]+1.\\n\\nW is the input volume\\nK is the Kernel size\\nP is the padding\\nS is the stride\\n\\n'"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# load the ResNet-18 model, with randomly initialized weights\n",
    "resnet18_random = models.resnet18(pretrained=False)\n",
    "# load the ResNet-18 model, with weights pretrained on ImageNet\n",
    "resnet18_pretrained = models.resnet18(pretrained=True)\n",
    "\n",
    "num_params = 0\n",
    "print(\"Model's parameters: \")\n",
    "for n, p in resnet18_pretrained.named_parameters():\n",
    "    print('\\t', n, ': ', p.size())\n",
    "    num_params += p.numel()\n",
    "print(\"Number of model parameters: \", num_params)\n",
    "\n",
    "\"\"\"\n",
    "you can use this formula [(W−K+2P)/S]+1.\n",
    "\n",
    "W is the input volume\n",
    "K is the Kernel size\n",
    "P is the padding\n",
    "S is the stride\n",
    "\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "transf = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                        std=[0.229, 0.224, 0.225])\n",
    "  ])\n",
    "# image = transf(Image.open(image_path))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# function counting the number of parameters and the number of trainable parameters of a model\n",
    "# optionally, it will also display the layers\n",
    "def check_model_parameters(model, display_layers=False):\n",
    "  num_params = 0\n",
    "  num_trainable_params = 0\n",
    "  if display_layers==True:\n",
    "    print(\"Model's parameters: \")\n",
    "  for n, p in model.named_parameters():\n",
    "      if display_layers == True:\n",
    "        print('\\t', n, ': ', p.size())\n",
    "      num_params += p.numel()\n",
    "      if p.requires_grad:\n",
    "        num_trainable_params += p.numel()\n",
    "  print(\"Number of model parameters: \", num_params)\n",
    "  print(\"Number of trainable parameters: \", num_trainable_params)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of model parameters:  11689512\n",
      "Number of trainable parameters:  11689512\n",
      "Number of model parameters:  11689512\n",
      "Number of trainable parameters:  0\n"
     ]
    }
   ],
   "source": [
    "# freeze the model parameters\n",
    "import torchvision.models as models\n",
    "# load the ResNet-18 model, with weights pretrained on ImageNet\n",
    "resnet18_pretrained = models.resnet18(pretrained=True)\n",
    "\n",
    "# check the number of parameters and the number of trainable parameters\n",
    "check_model_parameters(resnet18_pretrained, display_layers=False)\n",
    "\n",
    "# freeze all the layers\n",
    "for param in resnet18_pretrained.parameters():\n",
    "  param.requires_grad = False\n",
    "\n",
    "# check the number of parameters and the number of trainable parameters\n",
    "check_model_parameters(resnet18_pretrained, display_layers=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's parameters: \n",
      "\t conv1.weight :  torch.Size([64, 3, 7, 7])\n",
      "\t bn1.weight :  torch.Size([64])\n",
      "\t bn1.bias :  torch.Size([64])\n",
      "\t layer1.0.conv1.weight :  torch.Size([64, 64, 3, 3])\n",
      "\t layer1.0.bn1.weight :  torch.Size([64])\n",
      "\t layer1.0.bn1.bias :  torch.Size([64])\n",
      "\t layer1.0.conv2.weight :  torch.Size([64, 64, 3, 3])\n",
      "\t layer1.0.bn2.weight :  torch.Size([64])\n",
      "\t layer1.0.bn2.bias :  torch.Size([64])\n",
      "\t layer1.1.conv1.weight :  torch.Size([64, 64, 3, 3])\n",
      "\t layer1.1.bn1.weight :  torch.Size([64])\n",
      "\t layer1.1.bn1.bias :  torch.Size([64])\n",
      "\t layer1.1.conv2.weight :  torch.Size([64, 64, 3, 3])\n",
      "\t layer1.1.bn2.weight :  torch.Size([64])\n",
      "\t layer1.1.bn2.bias :  torch.Size([64])\n",
      "\t layer2.0.conv1.weight :  torch.Size([128, 64, 3, 3])\n",
      "\t layer2.0.bn1.weight :  torch.Size([128])\n",
      "\t layer2.0.bn1.bias :  torch.Size([128])\n",
      "\t layer2.0.conv2.weight :  torch.Size([128, 128, 3, 3])\n",
      "\t layer2.0.bn2.weight :  torch.Size([128])\n",
      "\t layer2.0.bn2.bias :  torch.Size([128])\n",
      "\t layer2.0.downsample.0.weight :  torch.Size([128, 64, 1, 1])\n",
      "\t layer2.0.downsample.1.weight :  torch.Size([128])\n",
      "\t layer2.0.downsample.1.bias :  torch.Size([128])\n",
      "\t layer2.1.conv1.weight :  torch.Size([128, 128, 3, 3])\n",
      "\t layer2.1.bn1.weight :  torch.Size([128])\n",
      "\t layer2.1.bn1.bias :  torch.Size([128])\n",
      "\t layer2.1.conv2.weight :  torch.Size([128, 128, 3, 3])\n",
      "\t layer2.1.bn2.weight :  torch.Size([128])\n",
      "\t layer2.1.bn2.bias :  torch.Size([128])\n",
      "\t layer3.0.conv1.weight :  torch.Size([256, 128, 3, 3])\n",
      "\t layer3.0.bn1.weight :  torch.Size([256])\n",
      "\t layer3.0.bn1.bias :  torch.Size([256])\n",
      "\t layer3.0.conv2.weight :  torch.Size([256, 256, 3, 3])\n",
      "\t layer3.0.bn2.weight :  torch.Size([256])\n",
      "\t layer3.0.bn2.bias :  torch.Size([256])\n",
      "\t layer3.0.downsample.0.weight :  torch.Size([256, 128, 1, 1])\n",
      "\t layer3.0.downsample.1.weight :  torch.Size([256])\n",
      "\t layer3.0.downsample.1.bias :  torch.Size([256])\n",
      "\t layer3.1.conv1.weight :  torch.Size([256, 256, 3, 3])\n",
      "\t layer3.1.bn1.weight :  torch.Size([256])\n",
      "\t layer3.1.bn1.bias :  torch.Size([256])\n",
      "\t layer3.1.conv2.weight :  torch.Size([256, 256, 3, 3])\n",
      "\t layer3.1.bn2.weight :  torch.Size([256])\n",
      "\t layer3.1.bn2.bias :  torch.Size([256])\n",
      "\t layer4.0.conv1.weight :  torch.Size([512, 256, 3, 3])\n",
      "\t layer4.0.bn1.weight :  torch.Size([512])\n",
      "\t layer4.0.bn1.bias :  torch.Size([512])\n",
      "\t layer4.0.conv2.weight :  torch.Size([512, 512, 3, 3])\n",
      "\t layer4.0.bn2.weight :  torch.Size([512])\n",
      "\t layer4.0.bn2.bias :  torch.Size([512])\n",
      "\t layer4.0.downsample.0.weight :  torch.Size([512, 256, 1, 1])\n",
      "\t layer4.0.downsample.1.weight :  torch.Size([512])\n",
      "\t layer4.0.downsample.1.bias :  torch.Size([512])\n",
      "\t layer4.1.conv1.weight :  torch.Size([512, 512, 3, 3])\n",
      "\t layer4.1.bn1.weight :  torch.Size([512])\n",
      "\t layer4.1.bn1.bias :  torch.Size([512])\n",
      "\t layer4.1.conv2.weight :  torch.Size([512, 512, 3, 3])\n",
      "\t layer4.1.bn2.weight :  torch.Size([512])\n",
      "\t layer4.1.bn2.bias :  torch.Size([512])\n",
      "\t fc.weight :  torch.Size([2, 512])\n",
      "\t fc.bias :  torch.Size([2])\n",
      "Number of model parameters:  11177538\n",
      "Number of trainable parameters:  1026\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "resnet18_pretrained.fc = nn.Linear(in_features=512, out_features=2, bias=True)\n",
    "\n",
    "check_model_parameters(resnet18_pretrained, display_layers=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "def train_epoch(model, train_dataloader, loss_crt, optimizer, device):\n",
    "    \"\"\"\n",
    "    model: Model object\n",
    "    train_dataloader: DataLoader over the training dataset\n",
    "    loss_crt: loss function object\n",
    "    optimizer: Optimizer object\n",
    "    device: torch.device('cpu) or torch.device('cuda')\n",
    "\n",
    "    The function returns:\n",
    "     - the epoch training loss, which is an average over the individual batch\n",
    "       losses\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    epoch_accuracy = 0.0\n",
    "    num_batches = len(train_dataloader)\n",
    "    for batch_idx, batch in tqdm(enumerate(train_dataloader)):\n",
    "        # shape: batch_size x 1 x 28 x 28, batch_size x 1\n",
    "        # print(\"Train\")\n",
    "        # print(batch)\n",
    "        batch_img, batch_labels, _ = batch\n",
    "        # move data to GPU\n",
    "        batch_img = batch_img.to(device)\n",
    "        batch_labels = batch_labels.to(device)\n",
    "\n",
    "        # initialize as zeros all the gradients of the model\n",
    "        model.zero_grad()\n",
    "\n",
    "        # get predictions from the FORWARD pass\n",
    "        # shape: batch_size x 10\n",
    "        output = model(batch_img)\n",
    "\n",
    "        loss = loss_crt(output, batch_labels.squeeze())\n",
    "        loss_scalar = loss.item()\n",
    "\n",
    "        # BACKPROPAGATE the gradients\n",
    "        loss.backward()\n",
    "        # use the gradients to OPTIMISE the model\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss_scalar\n",
    "\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        epoch_accuracy += pred.eq(batch_labels.view_as(pred)).float().mean().item()\n",
    "\n",
    "    epoch_loss = epoch_loss/num_batches\n",
    "    epoch_accuracy = 100. * epoch_accuracy/num_batches\n",
    "    return epoch_loss, epoch_accuracy\n",
    "\n",
    "def eval_epoch(model, val_dataloader, loss_crt, device):\n",
    "    \"\"\"\n",
    "    model: Model object\n",
    "    val_dataloader: DataLoader over the validation dataset\n",
    "    loss_crt: loss function object\n",
    "    device: torch.device('cpu) or torch.device('cuda')\n",
    "\n",
    "    The function returns:\n",
    "     - the epoch validation loss, which is an average over the individual batch\n",
    "       losses\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    epoch_loss = 0.0\n",
    "    epoch_accuracy = 0.0\n",
    "    num_batches = len(val_dataloader)\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in tqdm(enumerate(val_dataloader)):\n",
    "            # print(\"Eval\")\n",
    "            # print(batch)\n",
    "            # shape: batch_size x 3 x 28 x 28, batch_size x 1\n",
    "            batch_img, batch_labels, _ = batch\n",
    "            current_batch_size = batch_img.size(0)\n",
    "\n",
    "            # move data to GPU\n",
    "            batch_img = batch_img.to(device)\n",
    "            batch_labels = batch_labels.to(device)\n",
    "\n",
    "            # batch_size x 10\n",
    "            output = model(batch_img)\n",
    "\n",
    "            loss = loss_crt(output, batch_labels.squeeze())\n",
    "            loss_scalar = loss.item()\n",
    "\n",
    "            epoch_loss += loss_scalar\n",
    "\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            epoch_accuracy += pred.eq(batch_labels.view_as(pred)).float().mean().item()\n",
    "\n",
    "    epoch_loss = epoch_loss/num_batches\n",
    "    epoch_accuracy = 100. * epoch_accuracy/num_batches\n",
    "    return epoch_loss, epoch_accuracy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "164it [02:18,  1.18it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[0;32mIn [37]\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     18\u001B[0m val_accuracies \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m     19\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m1\u001B[39m, num_epochs\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m):\n\u001B[0;32m---> 20\u001B[0m   train_loss, train_accuracy \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_epoch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mresnet18_pretrained\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mloss_criterion\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     21\u001B[0m   val_loss, val_accuracy \u001B[38;5;241m=\u001B[39m eval_epoch(resnet18_pretrained, id_val_loader, loss_criterion, device)\n\u001B[1;32m     22\u001B[0m   train_losses\u001B[38;5;241m.\u001B[39mappend(train_loss)\n",
      "Input \u001B[0;32mIn [36]\u001B[0m, in \u001B[0;36mtrain_epoch\u001B[0;34m(model, train_dataloader, loss_crt, optimizer, device)\u001B[0m\n\u001B[1;32m     15\u001B[0m epoch_accuracy \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.0\u001B[39m\n\u001B[1;32m     16\u001B[0m num_batches \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(train_dataloader)\n\u001B[0;32m---> 17\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m batch_idx, batch \u001B[38;5;129;01min\u001B[39;00m tqdm(\u001B[38;5;28menumerate\u001B[39m(train_dataloader)):\n\u001B[1;32m     18\u001B[0m     \u001B[38;5;66;03m# shape: batch_size x 1 x 28 x 28, batch_size x 1\u001B[39;00m\n\u001B[1;32m     19\u001B[0m     \u001B[38;5;66;03m# print(\"Train\")\u001B[39;00m\n\u001B[1;32m     20\u001B[0m     \u001B[38;5;66;03m# print(batch)\u001B[39;00m\n\u001B[1;32m     21\u001B[0m     batch_img, batch_labels, _ \u001B[38;5;241m=\u001B[39m batch\n\u001B[1;32m     22\u001B[0m     \u001B[38;5;66;03m# move data to GPU\u001B[39;00m\n",
      "File \u001B[0;32m~/.pyenv/versions/nexgen/lib/python3.9/site-packages/tqdm/std.py:1195\u001B[0m, in \u001B[0;36mtqdm.__iter__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1192\u001B[0m time \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_time\n\u001B[1;32m   1194\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1195\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m obj \u001B[38;5;129;01min\u001B[39;00m iterable:\n\u001B[1;32m   1196\u001B[0m         \u001B[38;5;28;01myield\u001B[39;00m obj\n\u001B[1;32m   1197\u001B[0m         \u001B[38;5;66;03m# Update and possibly print the progressbar.\u001B[39;00m\n\u001B[1;32m   1198\u001B[0m         \u001B[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001B[39;00m\n",
      "File \u001B[0;32m~/.pyenv/versions/nexgen/lib/python3.9/site-packages/torch/utils/data/dataloader.py:521\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    519\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    520\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()\n\u001B[0;32m--> 521\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    522\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    523\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    524\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    525\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[0;32m~/.pyenv/versions/nexgen/lib/python3.9/site-packages/torch/utils/data/dataloader.py:561\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    559\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    560\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m--> 561\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m    562\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[1;32m    563\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data)\n",
      "File \u001B[0;32m~/.pyenv/versions/nexgen/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:49\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[0;34m(self, possibly_batched_index)\u001B[0m\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfetch\u001B[39m(\u001B[38;5;28mself\u001B[39m, possibly_batched_index):\n\u001B[1;32m     48\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mauto_collation:\n\u001B[0;32m---> 49\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[1;32m     50\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     51\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[0;32m~/.pyenv/versions/nexgen/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:49\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfetch\u001B[39m(\u001B[38;5;28mself\u001B[39m, possibly_batched_index):\n\u001B[1;32m     48\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mauto_collation:\n\u001B[0;32m---> 49\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[1;32m     50\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     51\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[0;32m~/.pyenv/versions/nexgen/lib/python3.9/site-packages/wilds/datasets/wilds_dataset.py:498\u001B[0m, in \u001B[0;36mWILDSSubset.__getitem__\u001B[0;34m(self, idx)\u001B[0m\n\u001B[1;32m    497\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__getitem__\u001B[39m(\u001B[38;5;28mself\u001B[39m, idx):\n\u001B[0;32m--> 498\u001B[0m     x, y, metadata \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mindices\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m\n\u001B[1;32m    499\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransform \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    500\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdo_transform_y:\n",
      "File \u001B[0;32m~/.pyenv/versions/nexgen/lib/python3.9/site-packages/wilds/datasets/wilds_dataset.py:32\u001B[0m, in \u001B[0;36mWILDSDataset.__getitem__\u001B[0;34m(self, idx)\u001B[0m\n\u001B[1;32m     29\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__getitem__\u001B[39m(\u001B[38;5;28mself\u001B[39m, idx):\n\u001B[1;32m     30\u001B[0m     \u001B[38;5;66;03m# Any transformations are handled by the WILDSSubset\u001B[39;00m\n\u001B[1;32m     31\u001B[0m     \u001B[38;5;66;03m# since different subsets (e.g., train vs test) might have different transforms\u001B[39;00m\n\u001B[0;32m---> 32\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_input\u001B[49m\u001B[43m(\u001B[49m\u001B[43midx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     33\u001B[0m     y \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39my_array[idx]\n\u001B[1;32m     34\u001B[0m     metadata \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmetadata_array[idx]\n",
      "File \u001B[0;32m~/.pyenv/versions/nexgen/lib/python3.9/site-packages/wilds/datasets/camelyon17_dataset.py:138\u001B[0m, in \u001B[0;36mCamelyon17Dataset.get_input\u001B[0;34m(self, idx)\u001B[0m\n\u001B[1;32m    132\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    133\u001B[0m \u001B[38;5;124;03mReturns x for a given idx.\u001B[39;00m\n\u001B[1;32m    134\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    135\u001B[0m img_filename \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(\n\u001B[1;32m    136\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdata_dir,\n\u001B[1;32m    137\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_input_array[idx])\n\u001B[0;32m--> 138\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[43mImage\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg_filename\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconvert\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mRGB\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    139\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m x\n",
      "File \u001B[0;32m~/.pyenv/versions/nexgen/lib/python3.9/site-packages/PIL/Image.py:889\u001B[0m, in \u001B[0;36mImage.convert\u001B[0;34m(self, mode, matrix, dither, palette, colors)\u001B[0m\n\u001B[1;32m    847\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mconvert\u001B[39m(\u001B[38;5;28mself\u001B[39m, mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, matrix\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, dither\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, palette\u001B[38;5;241m=\u001B[39mWEB, colors\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m256\u001B[39m):\n\u001B[1;32m    848\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    849\u001B[0m \u001B[38;5;124;03m    Returns a converted copy of this image. For the \"P\" mode, this\u001B[39;00m\n\u001B[1;32m    850\u001B[0m \u001B[38;5;124;03m    method translates pixels through the palette.  If mode is\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    886\u001B[0m \u001B[38;5;124;03m    :returns: An :py:class:`~PIL.Image.Image` object.\u001B[39;00m\n\u001B[1;32m    887\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 889\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    891\u001B[0m     has_transparency \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minfo\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtransparency\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    892\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m mode \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmode \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mP\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m    893\u001B[0m         \u001B[38;5;66;03m# determine default mode\u001B[39;00m\n",
      "File \u001B[0;32m~/.pyenv/versions/nexgen/lib/python3.9/site-packages/PIL/ImageFile.py:235\u001B[0m, in \u001B[0;36mImageFile.load\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    233\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m    234\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 235\u001B[0m         s \u001B[38;5;241m=\u001B[39m \u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecodermaxblock\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m (\u001B[38;5;167;01mIndexError\u001B[39;00m, struct\u001B[38;5;241m.\u001B[39merror) \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    237\u001B[0m         \u001B[38;5;66;03m# truncated png/gif\u001B[39;00m\n\u001B[1;32m    238\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m LOAD_TRUNCATED_IMAGES:\n",
      "File \u001B[0;32m~/.pyenv/versions/nexgen/lib/python3.9/site-packages/PIL/PngImagePlugin.py:919\u001B[0m, in \u001B[0;36mPngImageFile.load_read\u001B[0;34m(self, read_bytes)\u001B[0m\n\u001B[1;32m    915\u001B[0m     read_bytes \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mmin\u001B[39m(read_bytes, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m__idat)\n\u001B[1;32m    917\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m__idat \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m__idat \u001B[38;5;241m-\u001B[39m read_bytes\n\u001B[0;32m--> 919\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43mread_bytes\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "resnet18_pretrained.to(device)\n",
    "\n",
    "# create a SGD optimizer\n",
    "optimizer = torch.optim.SGD(resnet18_pretrained.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "# set up loss function\n",
    "loss_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# evaluate the initial model\n",
    "# val_loss, al_accuracy = eval_epoch(resnet18_pretrained, id_val_loader, loss_criterion, device)\n",
    "# print('Validation performance before finetuning -- loss: %10.8f, accuracy: %10.8f'%(val_loss, val_accuracy))\n",
    "\n",
    "# finetune the model\n",
    "num_epochs = 5\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "for epoch in range(1, num_epochs+1):\n",
    "  train_loss, train_accuracy = train_epoch(resnet18_pretrained, train_loader, loss_criterion, optimizer, device)\n",
    "  val_loss, val_accuracy = eval_epoch(resnet18_pretrained, id_val_loader, loss_criterion, device)\n",
    "  train_losses.append(train_loss)\n",
    "  val_losses.append(val_loss)\n",
    "  train_accuracies.append(train_accuracy)\n",
    "  val_accuracies.append(val_accuracy)\n",
    "  print('\\nEpoch %d'%(epoch))\n",
    "  print('train loss: %10.8f, accuracy: %10.8f'%(train_loss, train_accuracy))\n",
    "  print('val loss: %10.8f, accuracy: %10.8f'%(val_loss, val_accuracy))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}