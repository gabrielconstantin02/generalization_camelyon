{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "id": "iwSgBXS4WnPJ"
   },
   "outputs": [],
   "source": [
    "# !pip install wilds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from wilds import get_dataset\n",
    "from wilds.common.data_loaders import get_train_loader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from wilds.common.data_loaders import get_eval_loader\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "fbFdMbP_WnPK"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Load the full dataset, and download it if necessary\n",
    "dataset = get_dataset(dataset=\"camelyon17\", download=False)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "bAbxogzDWnPL"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99804\n"
     ]
    },
    {
     "data": {
      "text/plain": "'\\n# Train loop\\nfor labeled_batch, unlabeled_batch in zip(train_loader, unlabeled_loader):\\n    x, y, metadata = labeled_batch\\n    unlabeled_x, unlabeled_metadata = unlabeled_batch\\n    ...\\n'"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "FRACTION = 0.33\n",
    "\n",
    "# Get the training set\n",
    "train_data = dataset.get_subset(\n",
    "    \"train\",\n",
    "    frac = FRACTION,\n",
    "    transform=transforms.Compose(\n",
    "        [\n",
    "         transforms.ToTensor(),\n",
    "         transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                        std=[0.229, 0.224, 0.225])]\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(len(train_data)) #302436 initially\n",
    "\n",
    "\"\"\"\n",
    "# (Optional) Load unlabeled data\n",
    "dataset = get_dataset(dataset=\"camelyon17\", download=True, unlabeled=True)\n",
    "unlabeled_data = dataset.get_subset(\n",
    "    \"test_unlabeled\",\n",
    "    transform=transforms.Compose(\n",
    "        [transforms.Resize((448, 448)), transforms.ToTensor()]\n",
    "    ),\n",
    ")\n",
    "unlabeled_loader = get_train_loader(\"standard\", unlabeled_data, batch_size=16)\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "# Train loop\n",
    "for labeled_batch, unlabeled_batch in zip(train_loader, unlabeled_loader):\n",
    "    x, y, metadata = labeled_batch\n",
    "    unlabeled_x, unlabeled_metadata = unlabeled_batch\n",
    "    ...\n",
    "\"\"\""
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "vkrWsMbmWnPL",
    "outputId": "464c21c8-d58b-428e-8d39-4973e48dfdfe"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11075\n"
     ]
    }
   ],
   "source": [
    "# Get the id_val set\n",
    "id_val_data = dataset.get_subset(\n",
    "    \"id_val\",\n",
    "    frac = FRACTION,\n",
    "    transform=transforms.Compose(\n",
    "        [\n",
    "         transforms.ToTensor(),\n",
    "         transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                        std=[0.229, 0.224, 0.225])]\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(len(id_val_data))"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7bQG8UEhWnPM",
    "outputId": "e8b9ea11-b100-4370-a668-58f2f343884f"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11518\n"
     ]
    }
   ],
   "source": [
    "# Get the val set\n",
    "val_data = dataset.get_subset(\n",
    "    \"val\",\n",
    "    frac = FRACTION,\n",
    "    transform=transforms.Compose(\n",
    "        [\n",
    "         transforms.ToTensor(),\n",
    "         transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                        std=[0.229, 0.224, 0.225])]\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(len(val_data))"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "84YtJhH4WnPM",
    "outputId": "0871738a-4c3e-44fd-b4e1-5a871084d0e9"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28068\n"
     ]
    }
   ],
   "source": [
    "# Get the test set\n",
    "test_data = dataset.get_subset(\n",
    "    \"test\",\n",
    "    frac = FRACTION,\n",
    "    transform=transforms.Compose(\n",
    "        [\n",
    "         transforms.ToTensor(),\n",
    "         transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                        std=[0.229, 0.224, 0.225])]\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(len(test_data))\n"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uD1XSgxnWnPN",
    "outputId": "7f2fed59-1ac9-408c-c815-10ad2dfb1df7"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "print(test_data[0])"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MBsZzsWSBO2G",
    "outputId": "434f1f78-b6b5-4c8b-b137-b3f35d55a76f"
   },
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[ 0.2282,  0.0569,  0.6221,  ...,  0.1083,  0.1254,  0.0741],\n",
      "         [ 0.1254,  0.1426,  0.9474,  ...,  0.2796,  0.3309,  0.1768],\n",
      "         [-0.2171,  0.0398,  1.3413,  ...,  0.1426,  0.2453,  0.0227],\n",
      "         ...,\n",
      "         [-0.1657,  0.9817,  1.5982,  ...,  1.3070,  1.4612,  1.5297],\n",
      "         [-0.2856,  0.6221,  1.5982,  ...,  1.2899,  1.2385,  1.7865],\n",
      "         [ 0.5364,  0.4851,  1.0673,  ...,  1.0844,  1.1700,  1.1872]],\n",
      "\n",
      "        [[-0.6527, -0.4776, -0.5476,  ..., -0.4601, -0.5826, -0.4251],\n",
      "         [-0.6877, -0.6702, -0.2325,  ..., -0.3375, -0.4601, -0.4251],\n",
      "         [-0.8452, -0.7052,  0.0476,  ..., -0.4251, -0.4951, -0.5476],\n",
      "         ...,\n",
      "         [-1.0728,  0.2402,  1.0805,  ...,  0.1877,  0.3978,  0.9405],\n",
      "         [-0.7927, -0.3025,  0.7654,  ...,  0.1352,  0.2577,  0.2752],\n",
      "         [-0.3375, -0.3200,  0.6954,  ..., -0.0399,  0.0476,  0.0301]],\n",
      "\n",
      "        [[ 1.1411,  1.2457,  1.2805,  ...,  1.2457,  1.3328,  1.3154],\n",
      "         [ 1.2108,  1.2108,  1.3154,  ...,  1.3328,  1.4025,  1.2282],\n",
      "         [ 0.9842,  1.3154,  1.4722,  ...,  1.3154,  1.4025,  1.1585],\n",
      "         ...,\n",
      "         [ 0.8448,  1.8034,  2.1694,  ...,  1.6640,  1.8905,  2.2043],\n",
      "         [ 0.8274,  1.3677,  2.1694,  ...,  1.5942,  1.5594,  1.8208],\n",
      "         [ 1.4374,  1.6814,  1.8034,  ...,  1.6291,  1.6117,  1.6814]]]), tensor(1), tensor([ 2, 20,  1,  0]))\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "print(val_data[0])"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wh8tjoQVC93N",
    "outputId": "f6ea0de9-a739-4a2c-8967-988b8a26bb68"
   },
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[ 0.6049,  0.7591,  0.8276,  ...,  0.3823,  0.3652,  0.5536],\n",
      "         [ 0.6392,  0.5536,  0.5022,  ...,  0.4166,  0.3823,  0.4679],\n",
      "         [ 0.4679,  0.2624,  0.1426,  ...,  0.2967,  0.3138,  0.5364],\n",
      "         ...,\n",
      "         [ 0.1939,  0.2453,  0.0741,  ..., -0.1314, -0.1143,  0.1083],\n",
      "         [ 0.0398,  0.0741, -0.0629,  ..., -0.2171, -0.0458,  0.0741],\n",
      "         [-0.1657, -0.1657, -0.2684,  ...,  0.0569,  0.3309,  0.3994]],\n",
      "\n",
      "        [[-0.2500, -0.0049,  0.0651,  ..., -0.7927, -0.7052, -0.3200],\n",
      "         [-0.3025, -0.2850, -0.2850,  ..., -0.7577, -0.7577, -0.4776],\n",
      "         [-0.4601, -0.6176, -0.7577,  ..., -0.7227, -0.7227, -0.4251],\n",
      "         ...,\n",
      "         [-0.8102, -0.8277, -0.9153,  ..., -1.0028, -1.0378, -0.8277],\n",
      "         [-0.8803, -0.9153, -1.0028,  ..., -1.0203, -0.8277, -0.6702],\n",
      "         [-1.0728, -1.1429, -1.2129,  ..., -0.8452, -0.4776, -0.3200]],\n",
      "\n",
      "        [[ 0.6705,  0.8099,  0.8797,  ...,  0.2173,  0.2696,  0.5659],\n",
      "         [ 0.6008,  0.5659,  0.5485,  ...,  0.2522,  0.3045,  0.5311],\n",
      "         [ 0.5136,  0.3916,  0.3393,  ...,  0.2348,  0.2696,  0.5136],\n",
      "         ...,\n",
      "         [ 0.2173,  0.2173,  0.1476,  ...,  0.0953,  0.0779,  0.2348],\n",
      "         [ 0.1128,  0.0779,  0.0082,  ...,  0.0082,  0.1651,  0.2348],\n",
      "         [-0.0092, -0.0964, -0.1312,  ...,  0.2173,  0.5136,  0.5834]]]), tensor(1), tensor([ 1, 10,  1,  0]))\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "transformed_val_data = []\n",
    "for idx in range(len(val_data)):\n",
    "  transformed_val_data.append((val_data[idx][0], torch.tensor(2) if val_data[idx][1].item() == 0 else torch.tensor(3), val_data[idx][2]))"
   ],
   "metadata": {
    "id": "ovqRw48QBxQT"
   },
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(transformed_val_data[0])"
   ],
   "metadata": {
    "id": "BqtKW-3OC6a9",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "ce1638cb-f126-491a-fc53-0d218ae9fea0"
   },
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[ 0.6049,  0.7591,  0.8276,  ...,  0.3823,  0.3652,  0.5536],\n",
      "         [ 0.6392,  0.5536,  0.5022,  ...,  0.4166,  0.3823,  0.4679],\n",
      "         [ 0.4679,  0.2624,  0.1426,  ...,  0.2967,  0.3138,  0.5364],\n",
      "         ...,\n",
      "         [ 0.1939,  0.2453,  0.0741,  ..., -0.1314, -0.1143,  0.1083],\n",
      "         [ 0.0398,  0.0741, -0.0629,  ..., -0.2171, -0.0458,  0.0741],\n",
      "         [-0.1657, -0.1657, -0.2684,  ...,  0.0569,  0.3309,  0.3994]],\n",
      "\n",
      "        [[-0.2500, -0.0049,  0.0651,  ..., -0.7927, -0.7052, -0.3200],\n",
      "         [-0.3025, -0.2850, -0.2850,  ..., -0.7577, -0.7577, -0.4776],\n",
      "         [-0.4601, -0.6176, -0.7577,  ..., -0.7227, -0.7227, -0.4251],\n",
      "         ...,\n",
      "         [-0.8102, -0.8277, -0.9153,  ..., -1.0028, -1.0378, -0.8277],\n",
      "         [-0.8803, -0.9153, -1.0028,  ..., -1.0203, -0.8277, -0.6702],\n",
      "         [-1.0728, -1.1429, -1.2129,  ..., -0.8452, -0.4776, -0.3200]],\n",
      "\n",
      "        [[ 0.6705,  0.8099,  0.8797,  ...,  0.2173,  0.2696,  0.5659],\n",
      "         [ 0.6008,  0.5659,  0.5485,  ...,  0.2522,  0.3045,  0.5311],\n",
      "         [ 0.5136,  0.3916,  0.3393,  ...,  0.2348,  0.2696,  0.5136],\n",
      "         ...,\n",
      "         [ 0.2173,  0.2173,  0.1476,  ...,  0.0953,  0.0779,  0.2348],\n",
      "         [ 0.1128,  0.0779,  0.0082,  ...,  0.0082,  0.1651,  0.2348],\n",
      "         [-0.0092, -0.0964, -0.1312,  ...,  0.2173,  0.5136,  0.5834]]]), tensor(3), tensor([ 1, 10,  1,  0]))\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# for idx in range(len(train_data)):\n",
    "  # transformed_val_data.append(train_data[idx])"
   ],
   "metadata": {
    "id": "meayDFyPD72P"
   },
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# print(len(transformed_val_data))"
   ],
   "metadata": {
    "id": "i-er1YtCENPh"
   },
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "resnet18_pretrained = models.resnet18(pretrained=True)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "LK3oDNJaWnPN"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "num_epochs = 5"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "snTWKT8NWnPN"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "resnet18_pretrained.fc = nn.Linear(in_features=512, out_features=2, bias=True)\n",
    "resnet18_pretrained.to(device)\n",
    "# resnet18_pretrained.load_state_dict(torch.load(\"resnet18_pretrained_all_grad.pt\"))\n",
    "def remove_classification_head(model):\n",
    "    modules = list(model.children())[:-1]\n",
    "    model = nn.Sequential(*modules)\n",
    "    return model\n",
    "\n",
    "resnet18_pretrained = remove_classification_head(resnet18_pretrained)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "ZyzoVoLdWnPO"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class SiameseNetworkDataset(Dataset):\n",
    "    def __init__(self,iid_dataset, ood_dataset=None):\n",
    "        self.data = iid_dataset\n",
    "        self.ood_data = ood_dataset\n",
    "        \n",
    "    def __getitem__(self,index):\n",
    "        img0_tuple = random.choice(self.data)\n",
    "\n",
    "        #We need to approximately 50% of images to be in the same class\n",
    "        should_get_same_class = random.randint(0,1) \n",
    "        if should_get_same_class == 1:\n",
    "            while True:\n",
    "                #Look untill the same class image is found\n",
    "                img1_tuple = random.choice(self.data) \n",
    "                if img0_tuple[1] == img1_tuple[1]:\n",
    "                    break\n",
    "        else:\n",
    "            should_get_ood = random.randint(0, 1)\n",
    "            if should_get_ood == 1 and self.ood_data is not None:\n",
    "                while True:\n",
    "                    #Look untill a different class image is found\n",
    "                    img1_tuple = random.choice(self.ood_data)\n",
    "                    if img0_tuple[1] != img1_tuple[1]:\n",
    "                        break\n",
    "            else:\n",
    "                while True:\n",
    "                    #Look untill a different class image is found\n",
    "                    img1_tuple = random.choice(self.data)\n",
    "                    if img0_tuple[1] != img1_tuple[1]:\n",
    "                        break\n",
    "\n",
    "        img0 = img0_tuple[0]\n",
    "        img1 = img1_tuple[0]\n",
    "        \n",
    "        return img0, img1, torch.from_numpy(np.array([int(img1_tuple[1] != img0_tuple[1])], dtype=np.float32))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ],
   "metadata": {
    "id": "kcIDiIYgVqm9"
   },
   "execution_count": 17,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "import random"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "Y8DYeBZxWnPO"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "train_dataset = SiameseNetworkDataset(train_data, transformed_val_data)\n",
    "id_val_dataset = SiameseNetworkDataset(id_val_data)\n",
    "val_dataset = SiameseNetworkDataset(val_data)"
   ],
   "metadata": {
    "id": "29wbM5M6W0kg"
   },
   "execution_count": 19,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "test_dataset = SiameseNetworkDataset(test_data)"
   ],
   "metadata": {
    "id": "MzFfVZYbIqAc"
   },
   "execution_count": 20,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#create the Siamese Neural Network\n",
    "class SiameseNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "\n",
    "        # Setting up the Sequential of CNN Layers\n",
    "        self.embed = resnet18_pretrained\n",
    "\n",
    "        # Setting up the Fully Connected Layers\n",
    "        self.fc1 = nn.Sequential(            \n",
    "            nn.Linear(512,2)\n",
    "        )\n",
    "        \n",
    "    def forward_once(self, x):\n",
    "        # This function will be called for both images\n",
    "        # Its output is used to determine the similiarity\n",
    "        output = self.embed(x)\n",
    "        output = output.view(output.size()[0], -1)\n",
    "        output = self.fc1(output)\n",
    "        return output\n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        # In this function we pass in both images and obtain both vectors\n",
    "        # which are returned\n",
    "        output1 = self.forward_once(input1)\n",
    "        output2 = self.forward_once(input2)\n",
    "\n",
    "        return output1, output2"
   ],
   "metadata": {
    "id": "bUl6UWjlsZPY"
   },
   "execution_count": 21,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Define the Contrastive Loss Function\n",
    "class ContrastiveLoss(torch.nn.Module):\n",
    "    def __init__(self, margin=2.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, output1, output2, label):\n",
    "      # Calculate the euclidean distance and calculate the contrastive loss\n",
    "      euclidean_distance = F.pairwise_distance(output1, output2, keepdim = True)\n",
    "\n",
    "      loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_distance, 2) +\n",
    "                                    (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))\n",
    "\n",
    "\n",
    "      return loss_contrastive"
   ],
   "metadata": {
    "id": "SF8tlRiRu87k"
   },
   "execution_count": 22,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "train_loader = DataLoader(train_dataset,\n",
    "                        shuffle=True,\n",
    "                        batch_size=BATCH_SIZE)\n",
    "\n",
    "id_val_loader = DataLoader(id_val_dataset,\n",
    "                        batch_size=BATCH_SIZE)\n",
    "\n",
    "net = SiameseNetwork().to(device)\n",
    "\n",
    "loss_criterion = ContrastiveLoss()\n",
    "\n",
    "optimizer = torch.optim.SGD(resnet18_pretrained.parameters(), lr=0.01, momentum=0.9)"
   ],
   "metadata": {
    "id": "WrrwT7zsvFIr"
   },
   "execution_count": 23,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def train_epoch(model, train_dataloader, loss_crt, optimizer, device):\n",
    "    \"\"\"\n",
    "    model: Model object\n",
    "    train_dataloader: DataLoader over the training dataset\n",
    "    loss_crt: loss function object\n",
    "    optimizer: Optimizer object\n",
    "    device: torch.device('cpu) or torch.device('cuda')\n",
    "\n",
    "    The function returns:\n",
    "     - the epoch training loss, which is an average over the individual batch\n",
    "       losses\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    num_batches = len(train_dataloader)\n",
    "    # Iterate over batches\n",
    "    for i, (img0, img1, label) in tqdm(enumerate(train_dataloader, 0)):\n",
    "\n",
    "        # Send the images and labels to CUDA\n",
    "        img0, img1, label = img0.to(device), img1.to(device), label.to(device)\n",
    "\n",
    "        # Zero the gradients\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Pass in the two images into the network and obtain two outputs\n",
    "        output1, output2 = model(img0, img1)\n",
    "\n",
    "        # Pass the outputs of the networks and label into the loss function\n",
    "        loss_contrastive = loss_crt(output1, output2, label)\n",
    "\n",
    "        epoch_loss += loss_contrastive.item()\n",
    "\n",
    "        # Calculate the backpropagation\n",
    "        loss_contrastive.backward()\n",
    "\n",
    "        # Optimize\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "    epoch_loss = epoch_loss/num_batches\n",
    "    return epoch_loss\n",
    "\n",
    "def eval_epoch(model, val_dataloader, loss_crt, device):\n",
    "    \"\"\"\n",
    "    model: Model object\n",
    "    val_dataloader: DataLoader over the validation dataset\n",
    "    loss_crt: loss function object\n",
    "    device: torch.device('cpu) or torch.device('cuda')\n",
    "\n",
    "    The function returns:\n",
    "     - the epoch validation loss, which is an average over the individual batch\n",
    "       losses\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    num_batches = len(val_dataloader)\n",
    "    with torch.no_grad():\n",
    "      # Iterate over batches\n",
    "      for i, (img0, img1, label) in tqdm(enumerate(val_dataloader, 0)):\n",
    "\n",
    "          # Send the images and labels to CUDA\n",
    "          img0, img1, label = img0.to(device), img1.to(device), label.to(device)\n",
    "\n",
    "          # Pass in the two images into the network and obtain two outputs\n",
    "          output1, output2 = model(img0, img1)\n",
    "\n",
    "          # Pass the outputs of the networks and label into the loss function\n",
    "          loss_contrastive = loss_crt(output1, output2, label)\n",
    "\n",
    "          epoch_loss += loss_contrastive.item()\n",
    "\n",
    "    epoch_loss = epoch_loss/num_batches\n",
    "  \n",
    "    return epoch_loss"
   ],
   "metadata": {
    "id": "ku6KqKdRtIRI"
   },
   "execution_count": 24,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "train_losses = []\n",
    "train_accuracies = []\n",
    "id_val_losses = []\n",
    "id_val_accuracies = []\n",
    "for epoch in range(1, num_epochs+1):\n",
    "  train_loss = train_epoch(net, train_loader, loss_criterion, optimizer, device)\n",
    "  val_loss = eval_epoch(net, id_val_loader, loss_criterion, device)\n",
    "  train_losses.append(train_loss)\n",
    "  id_val_losses.append(val_loss)\n",
    "  print('\\nEpoch %d'%(epoch))\n",
    "  print('train loss: %10.8f'%(train_loss))\n",
    "  print('id_val loss: %10.8f'%(val_loss))"
   ],
   "metadata": {
    "id": "dLwPgDEmuZdl"
   },
   "execution_count": 25,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2396it [51:19,  1.29s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[0;32mIn [25]\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      4\u001B[0m id_val_accuracies \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m1\u001B[39m, num_epochs\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m):\n\u001B[0;32m----> 6\u001B[0m   train_loss \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_epoch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnet\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mloss_criterion\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      7\u001B[0m   val_loss \u001B[38;5;241m=\u001B[39m eval_epoch(net, id_val_loader, loss_criterion, device)\n\u001B[1;32m      8\u001B[0m   train_losses\u001B[38;5;241m.\u001B[39mappend(train_loss)\n",
      "Input \u001B[0;32mIn [24]\u001B[0m, in \u001B[0;36mtrain_epoch\u001B[0;34m(model, train_dataloader, loss_crt, optimizer, device)\u001B[0m\n\u001B[1;32m     16\u001B[0m num_batches \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(train_dataloader)\n\u001B[1;32m     17\u001B[0m \u001B[38;5;66;03m# Iterate over batches\u001B[39;00m\n\u001B[0;32m---> 18\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, (img0, img1, label) \u001B[38;5;129;01min\u001B[39;00m tqdm(\u001B[38;5;28menumerate\u001B[39m(train_dataloader, \u001B[38;5;241m0\u001B[39m)):\n\u001B[1;32m     19\u001B[0m \n\u001B[1;32m     20\u001B[0m     \u001B[38;5;66;03m# Send the images and labels to CUDA\u001B[39;00m\n\u001B[1;32m     21\u001B[0m     img0, img1, label \u001B[38;5;241m=\u001B[39m img0\u001B[38;5;241m.\u001B[39mto(device), img1\u001B[38;5;241m.\u001B[39mto(device), label\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m     23\u001B[0m     \u001B[38;5;66;03m# Zero the gradients\u001B[39;00m\n",
      "File \u001B[0;32m~/.pyenv/versions/nexgen/lib/python3.9/site-packages/tqdm/std.py:1195\u001B[0m, in \u001B[0;36mtqdm.__iter__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1192\u001B[0m time \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_time\n\u001B[1;32m   1194\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1195\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m obj \u001B[38;5;129;01min\u001B[39;00m iterable:\n\u001B[1;32m   1196\u001B[0m         \u001B[38;5;28;01myield\u001B[39;00m obj\n\u001B[1;32m   1197\u001B[0m         \u001B[38;5;66;03m# Update and possibly print the progressbar.\u001B[39;00m\n\u001B[1;32m   1198\u001B[0m         \u001B[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001B[39;00m\n",
      "File \u001B[0;32m~/.pyenv/versions/nexgen/lib/python3.9/site-packages/torch/utils/data/dataloader.py:521\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    519\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    520\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()\n\u001B[0;32m--> 521\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    522\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    523\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    524\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    525\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[0;32m~/.pyenv/versions/nexgen/lib/python3.9/site-packages/torch/utils/data/dataloader.py:561\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    559\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    560\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m--> 561\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m    562\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[1;32m    563\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data)\n",
      "File \u001B[0;32m~/.pyenv/versions/nexgen/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:49\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[0;34m(self, possibly_batched_index)\u001B[0m\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfetch\u001B[39m(\u001B[38;5;28mself\u001B[39m, possibly_batched_index):\n\u001B[1;32m     48\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mauto_collation:\n\u001B[0;32m---> 49\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[1;32m     50\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     51\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[0;32m~/.pyenv/versions/nexgen/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:49\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfetch\u001B[39m(\u001B[38;5;28mself\u001B[39m, possibly_batched_index):\n\u001B[1;32m     48\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mauto_collation:\n\u001B[0;32m---> 49\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[1;32m     50\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     51\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "Input \u001B[0;32mIn [17]\u001B[0m, in \u001B[0;36mSiameseNetworkDataset.__getitem__\u001B[0;34m(self, index)\u001B[0m\n\u001B[1;32m     11\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m should_get_same_class \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m     12\u001B[0m     \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m     13\u001B[0m         \u001B[38;5;66;03m#Look untill the same class image is found\u001B[39;00m\n\u001B[0;32m---> 14\u001B[0m         img1_tuple \u001B[38;5;241m=\u001B[39m \u001B[43mrandom\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mchoice\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m \n\u001B[1;32m     15\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m img0_tuple[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m==\u001B[39m img1_tuple[\u001B[38;5;241m1\u001B[39m]:\n\u001B[1;32m     16\u001B[0m             \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "File \u001B[0;32m~/.pyenv/versions/3.9.9/lib/python3.9/random.py:346\u001B[0m, in \u001B[0;36mRandom.choice\u001B[0;34m(self, seq)\u001B[0m\n\u001B[1;32m    344\u001B[0m \u001B[38;5;124;03m\"\"\"Choose a random element from a non-empty sequence.\"\"\"\u001B[39;00m\n\u001B[1;32m    345\u001B[0m \u001B[38;5;66;03m# raises IndexError if seq is empty\u001B[39;00m\n\u001B[0;32m--> 346\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mseq\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_randbelow\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mseq\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m]\u001B[49m\n",
      "File \u001B[0;32m~/.pyenv/versions/nexgen/lib/python3.9/site-packages/wilds/datasets/wilds_dataset.py:498\u001B[0m, in \u001B[0;36mWILDSSubset.__getitem__\u001B[0;34m(self, idx)\u001B[0m\n\u001B[1;32m    497\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__getitem__\u001B[39m(\u001B[38;5;28mself\u001B[39m, idx):\n\u001B[0;32m--> 498\u001B[0m     x, y, metadata \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mindices\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m\n\u001B[1;32m    499\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransform \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    500\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdo_transform_y:\n",
      "File \u001B[0;32m~/.pyenv/versions/nexgen/lib/python3.9/site-packages/wilds/datasets/wilds_dataset.py:32\u001B[0m, in \u001B[0;36mWILDSDataset.__getitem__\u001B[0;34m(self, idx)\u001B[0m\n\u001B[1;32m     29\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__getitem__\u001B[39m(\u001B[38;5;28mself\u001B[39m, idx):\n\u001B[1;32m     30\u001B[0m     \u001B[38;5;66;03m# Any transformations are handled by the WILDSSubset\u001B[39;00m\n\u001B[1;32m     31\u001B[0m     \u001B[38;5;66;03m# since different subsets (e.g., train vs test) might have different transforms\u001B[39;00m\n\u001B[0;32m---> 32\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_input\u001B[49m\u001B[43m(\u001B[49m\u001B[43midx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     33\u001B[0m     y \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39my_array[idx]\n\u001B[1;32m     34\u001B[0m     metadata \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmetadata_array[idx]\n",
      "File \u001B[0;32m~/.pyenv/versions/nexgen/lib/python3.9/site-packages/wilds/datasets/camelyon17_dataset.py:138\u001B[0m, in \u001B[0;36mCamelyon17Dataset.get_input\u001B[0;34m(self, idx)\u001B[0m\n\u001B[1;32m    132\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    133\u001B[0m \u001B[38;5;124;03mReturns x for a given idx.\u001B[39;00m\n\u001B[1;32m    134\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    135\u001B[0m img_filename \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(\n\u001B[1;32m    136\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdata_dir,\n\u001B[1;32m    137\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_input_array[idx])\n\u001B[0;32m--> 138\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[43mImage\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg_filename\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mconvert(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mRGB\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m    139\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m x\n",
      "File \u001B[0;32m~/.pyenv/versions/nexgen/lib/python3.9/site-packages/PIL/Image.py:2962\u001B[0m, in \u001B[0;36mopen\u001B[0;34m(fp, mode, formats)\u001B[0m\n\u001B[1;32m   2959\u001B[0m     fp \u001B[38;5;241m=\u001B[39m io\u001B[38;5;241m.\u001B[39mBytesIO(fp\u001B[38;5;241m.\u001B[39mread())\n\u001B[1;32m   2960\u001B[0m     exclusive_fp \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m-> 2962\u001B[0m prefix \u001B[38;5;241m=\u001B[39m \u001B[43mfp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m16\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2964\u001B[0m preinit()\n\u001B[1;32m   2966\u001B[0m accept_warnings \u001B[38;5;241m=\u001B[39m []\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "val_loader = DataLoader(val_dataset,\n",
    "                        batch_size=BATCH_SIZE)"
   ],
   "metadata": {
    "id": "ZlKH0PnkAlch"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import torchvision\n",
    "from matplotlib.pyplot import imshow\n",
    "import torchvision.utils\n",
    "# Grab one image that we are going to test\n",
    "dataiter = iter(train_loader)\n",
    "x0, _, label1 = next(dataiter)\n",
    "dataiter = iter(id_val_loader)\n",
    "\n",
    "dissimilarity = [[], []]\n",
    "\n",
    "for i in range(5):\n",
    "    # Iterate over 5 images and test them with the first image (x0)\n",
    "    _, x1, label2 = next(dataiter)\n",
    "    \n",
    "    output1, output2 = net(x0.cuda(), x1.cuda())\n",
    "\n",
    "    euclidean_distance = F.pairwise_distance(output1, output2)\n",
    "    \"\"\"\n",
    "    talking on the phone\n",
    "    \"\"\"\n",
    "    for idx in range(BATCH_SIZE):\n",
    "      # print(label1[idx].item(), label2[idx].item())\n",
    "      # print(f'Dissimilarity: {euclidean_distance[idx].item():.2f}')\n",
    "      # print(\"\\n\")\n",
    "      dissimilarity[1 if label1[idx].item() == label2[idx].item() else 0].append(euclidean_distance[idx].item())\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "1CU1STLi2ihG"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "def calc_mean_std(arr):\n",
    "    arr = np.asarray(arr)\n",
    "    return arr.mean(), arr.std()\n",
    "\n",
    "\n",
    "print(calc_mean_std(dissimilarity[0]))\n",
    "print(calc_mean_std(dissimilarity[1]))"
   ],
   "metadata": {
    "id": "2rJXz1yIBgH8"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "dissimilarity = [[], []]\n",
    "dataiter = iter(val_loader)\n",
    "for i in range(5):\n",
    "    # Iterate over 5 images and test them with the first image (x0)\n",
    "    _, x1, label2 = next(dataiter)\n",
    "    \n",
    "    output1, output2 = net(x0.cuda(), x1.cuda())\n",
    "\n",
    "    euclidean_distance = F.pairwise_distance(output1, output2)\n",
    "  \n",
    "    for idx in range(BATCH_SIZE):\n",
    "      # print(label1[idx].item(), label2[idx].item())\n",
    "      # print(f'Dissimilarity: {euclidean_distance[idx].item():.2f}')\n",
    "      dissimilarity[1 if label1[idx].item() == label2[idx].item() else 0].append(euclidean_distance[idx].item())"
   ],
   "metadata": {
    "id": "9ROkdNBaBvmN"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(calc_mean_std(dissimilarity[0]))\n",
    "print(calc_mean_std(dissimilarity[1]))"
   ],
   "metadata": {
    "id": "RnJbuUF6B35P"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "test_loader = DataLoader(test_dataset,\n",
    "                        batch_size=BATCH_SIZE)"
   ],
   "metadata": {
    "id": "DDucR0yAIwrI"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "dissimilarity = [[], []]\n",
    "dataiter = iter(test_loader)\n",
    "for i in range(5):\n",
    "    # Iterate over 5 images and test them with the first image (x0)\n",
    "    _, x1, label2 = next(dataiter)\n",
    "    \n",
    "    output1, output2 = net(x0.cuda(), x1.cuda())\n",
    "\n",
    "    euclidean_distance = F.pairwise_distance(output1, output2)\n",
    "    \n",
    "    for idx in range(BATCH_SIZE):\n",
    "      # print(label1[idx].item(), label2[idx].item())\n",
    "      # print(f'Dissimilarity: {euclidean_distance[idx].item():.2f}')\n",
    "      dissimilarity[1 if label1[idx].item() == label2[idx].item() else 0].append(euclidean_distance[idx].item())"
   ],
   "metadata": {
    "id": "FJT8YCjMIkyT"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "torch.save(net.state_dict(), \"contrastive_resnet18_sgd.pt\")"
   ],
   "metadata": {
    "id": "pY35HFfMOCCM"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(calc_mean_std(dissimilarity[0]))\n",
    "print(calc_mean_std(dissimilarity[1]))"
   ],
   "metadata": {
    "id": "uUSjfYwGInZf"
   },
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "name": "contrastive_learning.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}