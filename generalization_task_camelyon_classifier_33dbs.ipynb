{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# !pip install wilds"
      ],
      "metadata": {
        "id": "jbSfV818kVDW"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "collapsed": true,
        "id": "2v8Jq3hYhofR"
      },
      "outputs": [],
      "source": [
        "from wilds import get_dataset\n",
        "from wilds.common.data_loaders import get_train_loader\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "from wilds.common.data_loaders import get_eval_loader\n",
        "import torch\n",
        "from torch import nn\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "outputs": [],
      "source": [
        "# Load the full dataset, and download it if necessary\n",
        "dataset = get_dataset(dataset=\"camelyon17\", download=False)\n"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "9ovJ8k49hofV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "99804\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n# Train loop\\nfor labeled_batch, unlabeled_batch in zip(train_loader, unlabeled_loader):\\n    x, y, metadata = labeled_batch\\n    unlabeled_x, unlabeled_metadata = unlabeled_batch\\n    ...\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 49
        }
      ],
      "source": [
        "BATCH_SIZE = 32\n",
        "FRACTION = 0.33\n",
        "\n",
        "# Get the training set\n",
        "train_data = dataset.get_subset(\n",
        "    \"train\",\n",
        "    frac = FRACTION,\n",
        "    transform=transforms.Compose(\n",
        "        [\n",
        "         transforms.ToTensor(),\n",
        "         transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                        std=[0.229, 0.224, 0.225])]\n",
        "    ),\n",
        ")\n",
        "\n",
        "print(len(train_data)) #302436 initially\n",
        "# Prepare the standard data loader\n",
        "train_loader = get_train_loader(\"standard\", train_data, batch_size=BATCH_SIZE)\n",
        "\n",
        "\"\"\"\n",
        "# (Optional) Load unlabeled data\n",
        "dataset = get_dataset(dataset=\"camelyon17\", download=True, unlabeled=True)\n",
        "unlabeled_data = dataset.get_subset(\n",
        "    \"test_unlabeled\",\n",
        "    transform=transforms.Compose(\n",
        "        [transforms.Resize((448, 448)), transforms.ToTensor()]\n",
        "    ),\n",
        ")\n",
        "unlabeled_loader = get_train_loader(\"standard\", unlabeled_data, batch_size=16)\n",
        "\"\"\"\n",
        "\"\"\"\n",
        "# Train loop\n",
        "for labeled_batch, unlabeled_batch in zip(train_loader, unlabeled_loader):\n",
        "    x, y, metadata = labeled_batch\n",
        "    unlabeled_x, unlabeled_metadata = unlabeled_batch\n",
        "    ...\n",
        "\"\"\""
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "idPjNiuQhofV",
        "outputId": "576b6c57-d563-40e7-f86e-d23b59a356ab"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11075\n"
          ]
        }
      ],
      "source": [
        "# Get the test set\n",
        "id_val_data = dataset.get_subset(\n",
        "    \"id_val\",\n",
        "    frac = FRACTION,\n",
        "    transform=transforms.Compose(\n",
        "        [\n",
        "         transforms.ToTensor(),\n",
        "         transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                        std=[0.229, 0.224, 0.225])]\n",
        "    ),\n",
        ")\n",
        "\n",
        "print(len(id_val_data))\n",
        "\n",
        "# Prepare the evaluation data loader\n",
        "id_val_loader = get_eval_loader(\"standard\", id_val_data, batch_size=BATCH_SIZE)\n"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-KXocT0mhofX",
        "outputId": "273ff5f5-1c84-4114-82f4-8b2b92f6a4fa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11518\n"
          ]
        }
      ],
      "source": [
        "# Get the test set\n",
        "val_data = dataset.get_subset(\n",
        "    \"val\",\n",
        "    frac = FRACTION,\n",
        "    transform=transforms.Compose(\n",
        "        [\n",
        "         transforms.ToTensor(),\n",
        "         transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                        std=[0.229, 0.224, 0.225])]\n",
        "    ),\n",
        ")\n",
        "\n",
        "print(len(val_data))\n",
        "\n",
        "# Prepare the evaluation data loader\n",
        "val_loader = get_eval_loader(\"standard\", val_data, batch_size=BATCH_SIZE)"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7825GiE2hofX",
        "outputId": "2ee4ca2e-335f-4bdd-9fab-c0660c7d7d7c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the test set\n",
        "test_data = dataset.get_subset(\n",
        "    \"test\",\n",
        "    frac = FRACTION,\n",
        "    transform=transforms.Compose(\n",
        "        [\n",
        "         transforms.ToTensor(),\n",
        "         transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                        std=[0.229, 0.224, 0.225])]\n",
        "    ),\n",
        ")\n",
        "\n",
        "print(len(test_data))\n",
        "\n",
        "# Prepare the evaluation data loader\n",
        "test_loader = get_eval_loader(\"standard\", test_data, batch_size=BATCH_SIZE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NXa_rPBB1mfI",
        "outputId": "884bff8c-4f2d-46e2-ad6c-07c8f9002637"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "28068\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model's parameters: \n",
            "\t conv1.weight :  torch.Size([64, 3, 7, 7])\n",
            "\t bn1.weight :  torch.Size([64])\n",
            "\t bn1.bias :  torch.Size([64])\n",
            "\t layer1.0.conv1.weight :  torch.Size([64, 64, 3, 3])\n",
            "\t layer1.0.bn1.weight :  torch.Size([64])\n",
            "\t layer1.0.bn1.bias :  torch.Size([64])\n",
            "\t layer1.0.conv2.weight :  torch.Size([64, 64, 3, 3])\n",
            "\t layer1.0.bn2.weight :  torch.Size([64])\n",
            "\t layer1.0.bn2.bias :  torch.Size([64])\n",
            "\t layer1.1.conv1.weight :  torch.Size([64, 64, 3, 3])\n",
            "\t layer1.1.bn1.weight :  torch.Size([64])\n",
            "\t layer1.1.bn1.bias :  torch.Size([64])\n",
            "\t layer1.1.conv2.weight :  torch.Size([64, 64, 3, 3])\n",
            "\t layer1.1.bn2.weight :  torch.Size([64])\n",
            "\t layer1.1.bn2.bias :  torch.Size([64])\n",
            "\t layer2.0.conv1.weight :  torch.Size([128, 64, 3, 3])\n",
            "\t layer2.0.bn1.weight :  torch.Size([128])\n",
            "\t layer2.0.bn1.bias :  torch.Size([128])\n",
            "\t layer2.0.conv2.weight :  torch.Size([128, 128, 3, 3])\n",
            "\t layer2.0.bn2.weight :  torch.Size([128])\n",
            "\t layer2.0.bn2.bias :  torch.Size([128])\n",
            "\t layer2.0.downsample.0.weight :  torch.Size([128, 64, 1, 1])\n",
            "\t layer2.0.downsample.1.weight :  torch.Size([128])\n",
            "\t layer2.0.downsample.1.bias :  torch.Size([128])\n",
            "\t layer2.1.conv1.weight :  torch.Size([128, 128, 3, 3])\n",
            "\t layer2.1.bn1.weight :  torch.Size([128])\n",
            "\t layer2.1.bn1.bias :  torch.Size([128])\n",
            "\t layer2.1.conv2.weight :  torch.Size([128, 128, 3, 3])\n",
            "\t layer2.1.bn2.weight :  torch.Size([128])\n",
            "\t layer2.1.bn2.bias :  torch.Size([128])\n",
            "\t layer3.0.conv1.weight :  torch.Size([256, 128, 3, 3])\n",
            "\t layer3.0.bn1.weight :  torch.Size([256])\n",
            "\t layer3.0.bn1.bias :  torch.Size([256])\n",
            "\t layer3.0.conv2.weight :  torch.Size([256, 256, 3, 3])\n",
            "\t layer3.0.bn2.weight :  torch.Size([256])\n",
            "\t layer3.0.bn2.bias :  torch.Size([256])\n",
            "\t layer3.0.downsample.0.weight :  torch.Size([256, 128, 1, 1])\n",
            "\t layer3.0.downsample.1.weight :  torch.Size([256])\n",
            "\t layer3.0.downsample.1.bias :  torch.Size([256])\n",
            "\t layer3.1.conv1.weight :  torch.Size([256, 256, 3, 3])\n",
            "\t layer3.1.bn1.weight :  torch.Size([256])\n",
            "\t layer3.1.bn1.bias :  torch.Size([256])\n",
            "\t layer3.1.conv2.weight :  torch.Size([256, 256, 3, 3])\n",
            "\t layer3.1.bn2.weight :  torch.Size([256])\n",
            "\t layer3.1.bn2.bias :  torch.Size([256])\n",
            "\t layer4.0.conv1.weight :  torch.Size([512, 256, 3, 3])\n",
            "\t layer4.0.bn1.weight :  torch.Size([512])\n",
            "\t layer4.0.bn1.bias :  torch.Size([512])\n",
            "\t layer4.0.conv2.weight :  torch.Size([512, 512, 3, 3])\n",
            "\t layer4.0.bn2.weight :  torch.Size([512])\n",
            "\t layer4.0.bn2.bias :  torch.Size([512])\n",
            "\t layer4.0.downsample.0.weight :  torch.Size([512, 256, 1, 1])\n",
            "\t layer4.0.downsample.1.weight :  torch.Size([512])\n",
            "\t layer4.0.downsample.1.bias :  torch.Size([512])\n",
            "\t layer4.1.conv1.weight :  torch.Size([512, 512, 3, 3])\n",
            "\t layer4.1.bn1.weight :  torch.Size([512])\n",
            "\t layer4.1.bn1.bias :  torch.Size([512])\n",
            "\t layer4.1.conv2.weight :  torch.Size([512, 512, 3, 3])\n",
            "\t layer4.1.bn2.weight :  torch.Size([512])\n",
            "\t layer4.1.bn2.bias :  torch.Size([512])\n",
            "\t fc.weight :  torch.Size([1000, 512])\n",
            "\t fc.bias :  torch.Size([1000])\n",
            "Number of model parameters:  11689512\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nformula [(W−K+2P)/S]+1.\\n\\nW is the input volume\\nK is the Kernel size\\nP is the padding\\nS is the stride\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 53
        }
      ],
      "source": [
        "\n",
        "# load the ResNet-18 model, with weights pretrained on ImageNet\n",
        "resnet18_pretrained = models.resnet18(pretrained=True)\n",
        "\n",
        "num_params = 0\n",
        "print(\"Model's parameters: \")\n",
        "for n, p in resnet18_pretrained.named_parameters():\n",
        "    print('\\t', n, ': ', p.size())\n",
        "    num_params += p.numel()\n",
        "print(\"Number of model parameters: \", num_params)\n",
        "\n",
        "\"\"\"\n",
        "formula [(W−K+2P)/S]+1.\n",
        "\n",
        "W is the input volume\n",
        "K is the Kernel size\n",
        "P is the padding\n",
        "S is the stride\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "RaNKmMPUhofY",
        "outputId": "c8617911-3823-412f-8018-be8b16f138a7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "outputs": [],
      "source": [
        "# function counting the number of parameters and the number of trainable parameters of a model\n",
        "# optionally, it will also display the layers\n",
        "def check_model_parameters(model, display_layers=False):\n",
        "  num_params = 0\n",
        "  num_trainable_params = 0\n",
        "  if display_layers==True:\n",
        "    print(\"Model's parameters: \")\n",
        "  for n, p in model.named_parameters():\n",
        "      if display_layers == True:\n",
        "        print('\\t', n, ': ', p.size())\n",
        "      num_params += p.numel()\n",
        "      if p.requires_grad:\n",
        "        num_trainable_params += p.numel()\n",
        "  print(\"Number of model parameters: \", num_params)\n",
        "  print(\"Number of trainable parameters: \", num_trainable_params)"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "X-SlJVZkhofZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of model parameters:  11689512\n",
            "Number of trainable parameters:  11689512\n",
            "Number of model parameters:  11689512\n",
            "Number of trainable parameters:  0\n"
          ]
        }
      ],
      "source": [
        "# freeze the model parameters\n",
        "\n",
        "# check the number of parameters and the number of trainable parameters\n",
        "check_model_parameters(resnet18_pretrained, display_layers=False)\n",
        "\n",
        "# freeze all the layers\n",
        "for param in resnet18_pretrained.parameters():\n",
        "  param.requires_grad = False\n",
        "\n",
        "# check the number of parameters and the number of trainable parameters\n",
        "check_model_parameters(resnet18_pretrained, display_layers=False)"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sPIE8Xd3hofa",
        "outputId": "0927020f-33aa-4fa2-9bd1-203887276a59"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "num_epochs = 5"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "S8TUohu7hofa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model's parameters: \n",
            "\t conv1.weight :  torch.Size([64, 3, 7, 7])\n",
            "\t bn1.weight :  torch.Size([64])\n",
            "\t bn1.bias :  torch.Size([64])\n",
            "\t layer1.0.conv1.weight :  torch.Size([64, 64, 3, 3])\n",
            "\t layer1.0.bn1.weight :  torch.Size([64])\n",
            "\t layer1.0.bn1.bias :  torch.Size([64])\n",
            "\t layer1.0.conv2.weight :  torch.Size([64, 64, 3, 3])\n",
            "\t layer1.0.bn2.weight :  torch.Size([64])\n",
            "\t layer1.0.bn2.bias :  torch.Size([64])\n",
            "\t layer1.1.conv1.weight :  torch.Size([64, 64, 3, 3])\n",
            "\t layer1.1.bn1.weight :  torch.Size([64])\n",
            "\t layer1.1.bn1.bias :  torch.Size([64])\n",
            "\t layer1.1.conv2.weight :  torch.Size([64, 64, 3, 3])\n",
            "\t layer1.1.bn2.weight :  torch.Size([64])\n",
            "\t layer1.1.bn2.bias :  torch.Size([64])\n",
            "\t layer2.0.conv1.weight :  torch.Size([128, 64, 3, 3])\n",
            "\t layer2.0.bn1.weight :  torch.Size([128])\n",
            "\t layer2.0.bn1.bias :  torch.Size([128])\n",
            "\t layer2.0.conv2.weight :  torch.Size([128, 128, 3, 3])\n",
            "\t layer2.0.bn2.weight :  torch.Size([128])\n",
            "\t layer2.0.bn2.bias :  torch.Size([128])\n",
            "\t layer2.0.downsample.0.weight :  torch.Size([128, 64, 1, 1])\n",
            "\t layer2.0.downsample.1.weight :  torch.Size([128])\n",
            "\t layer2.0.downsample.1.bias :  torch.Size([128])\n",
            "\t layer2.1.conv1.weight :  torch.Size([128, 128, 3, 3])\n",
            "\t layer2.1.bn1.weight :  torch.Size([128])\n",
            "\t layer2.1.bn1.bias :  torch.Size([128])\n",
            "\t layer2.1.conv2.weight :  torch.Size([128, 128, 3, 3])\n",
            "\t layer2.1.bn2.weight :  torch.Size([128])\n",
            "\t layer2.1.bn2.bias :  torch.Size([128])\n",
            "\t layer3.0.conv1.weight :  torch.Size([256, 128, 3, 3])\n",
            "\t layer3.0.bn1.weight :  torch.Size([256])\n",
            "\t layer3.0.bn1.bias :  torch.Size([256])\n",
            "\t layer3.0.conv2.weight :  torch.Size([256, 256, 3, 3])\n",
            "\t layer3.0.bn2.weight :  torch.Size([256])\n",
            "\t layer3.0.bn2.bias :  torch.Size([256])\n",
            "\t layer3.0.downsample.0.weight :  torch.Size([256, 128, 1, 1])\n",
            "\t layer3.0.downsample.1.weight :  torch.Size([256])\n",
            "\t layer3.0.downsample.1.bias :  torch.Size([256])\n",
            "\t layer3.1.conv1.weight :  torch.Size([256, 256, 3, 3])\n",
            "\t layer3.1.bn1.weight :  torch.Size([256])\n",
            "\t layer3.1.bn1.bias :  torch.Size([256])\n",
            "\t layer3.1.conv2.weight :  torch.Size([256, 256, 3, 3])\n",
            "\t layer3.1.bn2.weight :  torch.Size([256])\n",
            "\t layer3.1.bn2.bias :  torch.Size([256])\n",
            "\t layer4.0.conv1.weight :  torch.Size([512, 256, 3, 3])\n",
            "\t layer4.0.bn1.weight :  torch.Size([512])\n",
            "\t layer4.0.bn1.bias :  torch.Size([512])\n",
            "\t layer4.0.conv2.weight :  torch.Size([512, 512, 3, 3])\n",
            "\t layer4.0.bn2.weight :  torch.Size([512])\n",
            "\t layer4.0.bn2.bias :  torch.Size([512])\n",
            "\t layer4.0.downsample.0.weight :  torch.Size([512, 256, 1, 1])\n",
            "\t layer4.0.downsample.1.weight :  torch.Size([512])\n",
            "\t layer4.0.downsample.1.bias :  torch.Size([512])\n",
            "\t layer4.1.conv1.weight :  torch.Size([512, 512, 3, 3])\n",
            "\t layer4.1.bn1.weight :  torch.Size([512])\n",
            "\t layer4.1.bn1.bias :  torch.Size([512])\n",
            "\t layer4.1.conv2.weight :  torch.Size([512, 512, 3, 3])\n",
            "\t layer4.1.bn2.weight :  torch.Size([512])\n",
            "\t layer4.1.bn2.bias :  torch.Size([512])\n",
            "\t fc.weight :  torch.Size([2, 512])\n",
            "\t fc.bias :  torch.Size([2])\n",
            "Number of model parameters:  11177538\n",
            "Number of trainable parameters:  1026\n"
          ]
        }
      ],
      "source": [
        "\n",
        "resnet18_pretrained.fc = nn.Linear(in_features=512, out_features=2, bias=True)\n",
        "\n",
        "check_model_parameters(resnet18_pretrained, display_layers=True)"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k1muosu4hofb",
        "outputId": "9f94a263-f46e-4d86-ca2c-a39aa5a503fa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "outputs": [],
      "source": [
        "def train_epoch(model, train_dataloader, loss_crt, optimizer, device):\n",
        "    \"\"\"\n",
        "    model: Model object\n",
        "    train_dataloader: DataLoader over the training dataset\n",
        "    loss_crt: loss function object\n",
        "    optimizer: Optimizer object\n",
        "    device: torch.device('cpu) or torch.device('cuda')\n",
        "\n",
        "    The function returns:\n",
        "     - the epoch training loss, which is an average over the individual batch\n",
        "       losses\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    epoch_loss = 0.0\n",
        "    epoch_accuracy = 0.0\n",
        "    num_batches = len(train_dataloader)\n",
        "    for batch_idx, batch in tqdm(enumerate(train_dataloader)):\n",
        "        # shape: batch_size x 1 x 28 x 28, batch_size x 1\n",
        "        # print(\"Train\")\n",
        "        # print(batch)\n",
        "        batch_img, batch_labels, _ = batch\n",
        "        # move data to GPU\n",
        "        batch_img = batch_img.to(device)\n",
        "        batch_labels = batch_labels.to(device)\n",
        "\n",
        "        # initialize as zeros all the gradients of the model\n",
        "        model.zero_grad()\n",
        "\n",
        "        # get predictions from the FORWARD pass\n",
        "        # shape: batch_size x 10\n",
        "        output = model(batch_img)\n",
        "\n",
        "        loss = loss_crt(output, batch_labels.squeeze())\n",
        "        loss_scalar = loss.item()\n",
        "\n",
        "        # BACKPROPAGATE the gradients\n",
        "        loss.backward()\n",
        "        # use the gradients to OPTIMISE the model\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss_scalar\n",
        "\n",
        "        pred = output.argmax(dim=1, keepdim=True)\n",
        "        epoch_accuracy += pred.eq(batch_labels.view_as(pred)).float().mean().item()\n",
        "\n",
        "    epoch_loss = epoch_loss/num_batches\n",
        "    epoch_accuracy = 100. * epoch_accuracy/num_batches\n",
        "    return epoch_loss, epoch_accuracy\n",
        "\n",
        "def eval_epoch(model, val_dataloader, loss_crt, device):\n",
        "    \"\"\"\n",
        "    model: Model object\n",
        "    val_dataloader: DataLoader over the validation dataset\n",
        "    loss_crt: loss function object\n",
        "    device: torch.device('cpu) or torch.device('cuda')\n",
        "\n",
        "    The function returns:\n",
        "     - the epoch validation loss, which is an average over the individual batch\n",
        "       losses\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    epoch_loss = 0.0\n",
        "    epoch_accuracy = 0.0\n",
        "    num_batches = len(val_dataloader)\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, batch in tqdm(enumerate(val_dataloader)):\n",
        "            # print(\"Eval\")\n",
        "            # print(batch)\n",
        "            # shape: batch_size x 3 x 28 x 28, batch_size x 1\n",
        "            batch_img, batch_labels, _ = batch\n",
        "            current_batch_size = batch_img.size(0)\n",
        "\n",
        "            # move data to GPU\n",
        "            batch_img = batch_img.to(device)\n",
        "            batch_labels = batch_labels.to(device)\n",
        "\n",
        "            # batch_size x 10\n",
        "            output = model(batch_img)\n",
        "\n",
        "            loss = loss_crt(output, batch_labels.squeeze())\n",
        "            loss_scalar = loss.item()\n",
        "\n",
        "            epoch_loss += loss_scalar\n",
        "\n",
        "            pred = output.argmax(dim=1, keepdim=True)\n",
        "            epoch_accuracy += pred.eq(batch_labels.view_as(pred)).float().mean().item()\n",
        "\n",
        "    epoch_loss = epoch_loss/num_batches\n",
        "    epoch_accuracy = 100. * epoch_accuracy/num_batches\n",
        "    return epoch_loss, epoch_accuracy"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "FgkgJVFXhofb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "3119it [04:23, 11.83it/s]\n",
            "347it [00:26, 13.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1\n",
            "train loss: 0.58285640, accuracy: 86.94937137\n",
            "id_val loss: 0.47694138, accuracy: 90.15670029\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "3119it [03:34, 14.52it/s]\n",
            "347it [00:21, 16.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 2\n",
            "train loss: 0.55098733, accuracy: 87.38578070\n",
            "id_val loss: 0.38916091, accuracy: 90.28578291\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "3119it [02:40, 19.46it/s]\n",
            "347it [00:17, 20.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 3\n",
            "train loss: 0.56883106, accuracy: 87.18010008\n",
            "id_val loss: 0.38958377, accuracy: 90.19572527\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "3119it [02:39, 19.55it/s]\n",
            "347it [00:16, 20.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 4\n",
            "train loss: 0.58089911, accuracy: 87.16621628\n",
            "id_val loss: 0.46813160, accuracy: 89.93455812\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "3119it [02:38, 19.73it/s]\n",
            "347it [00:16, 20.42it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 5\n",
            "train loss: 0.54611948, accuracy: 87.24708584\n",
            "id_val loss: 0.49794165, accuracy: 89.78746398\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "resnet18_pretrained.to(device)\n",
        "\n",
        "# create a SGD optimizer\n",
        "optimizer = torch.optim.SGD(resnet18_pretrained.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "# set up loss function\n",
        "loss_criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# evaluate the initial model\n",
        "# val_loss, al_accuracy = eval_epoch(resnet18_pretrained, id_val_loader, loss_criterion, device)\n",
        "# print('Validation performance before finetuning -- loss: %10.8f, accuracy: %10.8f'%(val_loss, val_accuracy))\n",
        "\n",
        "# finetune the model\n",
        "train_losses = []\n",
        "train_accuracies = []\n",
        "id_val_losses = []\n",
        "id_val_accuracies = []\n",
        "for epoch in range(1, num_epochs+1):\n",
        "  train_loss, train_accuracy = train_epoch(resnet18_pretrained, train_loader, loss_criterion, optimizer, device)\n",
        "  val_loss, val_accuracy = eval_epoch(resnet18_pretrained, id_val_loader, loss_criterion, device)\n",
        "  train_losses.append(train_loss)\n",
        "  id_val_losses.append(val_loss)\n",
        "  train_accuracies.append(train_accuracy)\n",
        "  id_val_accuracies.append(val_accuracy)\n",
        "  print('\\nEpoch %d'%(epoch))\n",
        "  print('train loss: %10.8f, accuracy: %10.8f'%(train_loss, train_accuracy))\n",
        "  print('id_val loss: %10.8f, accuracy: %10.8f'%(val_loss, val_accuracy))"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "daJBzTCNhofc",
        "outputId": "cb6170e9-73bc-4d14-f6d0-f5d2f5d12e1d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_eval_ood(model, loader, loss_criterion, device, eval_type):\n",
        "  losses = []\n",
        "  accuracies = []\n",
        "\n",
        "  loss, accuracy = eval_epoch(model, loader, loss_criterion, device)\n",
        "  losses.append(loss)\n",
        "  accuracies.append(accuracy)\n",
        "  print(eval_type + ' loss: %10.8f, accuracy: %10.8f'%(loss, accuracy))"
      ],
      "metadata": {
        "id": "ZoukrP_12g4y"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "360it [00:30, 11.82it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val loss: 1.14238133, accuracy: 81.17071761\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "run_eval_ood(resnet18_pretrained, val_loader, loss_criterion, device, \"val\")"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QMkwTCH6hofc",
        "outputId": "4a123d30-6005-4b98-f671-d49ad1ad1371"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run_eval_ood(resnet18_pretrained, test_loader, loss_criterion, device, \"test\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o9hGjljH11je",
        "outputId": "37451c42-ea14-45de-de45-d90e65651240"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "878it [01:13, 11.87it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test loss: 1.40432269, accuracy: 75.60506834\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of model parameters:  7978856\n",
            "Number of trainable parameters:  0\n"
          ]
        }
      ],
      "source": [
        "densenet121_pretrained = models.densenet121(pretrained=True)\n",
        "\n",
        "for param in densenet121_pretrained.parameters():\n",
        "  param.requires_grad = False\n",
        "\n",
        "check_model_parameters(densenet121_pretrained, display_layers=False)\n"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TNz_hqLIhofc",
        "outputId": "a24f4f96-561c-4b5d-eadf-3a693d52c615"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of model parameters:  6955906\n",
            "Number of trainable parameters:  2050\n"
          ]
        }
      ],
      "source": [
        "\n",
        "densenet121_pretrained.classifier = nn.Linear(in_features=1024, out_features=2, bias=True)\n",
        "check_model_parameters(densenet121_pretrained, display_layers=False)"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "igxeBmx2hofd",
        "outputId": "2cc78ecc-833f-46c6-d478-ef248d6bfd28"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "3119it [04:19, 12.04it/s]\n",
            "347it [00:25, 13.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1\n",
            "train loss: 0.50530743, accuracy: 89.04196057\n",
            "id_val loss: 0.33198662, accuracy: 92.79538905\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "3119it [03:53, 13.33it/s]\n",
            "347it [00:22, 15.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 2\n",
            "train loss: 0.47105016, accuracy: 89.97532405\n",
            "id_val loss: 0.34068261, accuracy: 91.66066282\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "3119it [03:52, 13.39it/s]\n",
            "347it [00:22, 15.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 3\n",
            "train loss: 0.44530267, accuracy: 90.13477305\n",
            "id_val loss: 0.31016541, accuracy: 93.11059078\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "3119it [03:51, 13.49it/s]\n",
            "347it [00:22, 15.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 4\n",
            "train loss: 0.49504382, accuracy: 89.91506550\n",
            "id_val loss: 0.32576735, accuracy: 92.39012968\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "3119it [03:52, 13.40it/s]\n",
            "347it [00:22, 15.12it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 5\n",
            "train loss: 0.47798958, accuracy: 90.19374342\n",
            "id_val loss: 0.34039470, accuracy: 93.31772334\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "densenet121_pretrained.to(device)\n",
        "\n",
        "# create a SGD optimizer\n",
        "optimizer = torch.optim.SGD(densenet121_pretrained.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "# set up loss function\n",
        "loss_criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# evaluate the initial model\n",
        "# val_loss, al_accuracy = eval_epoch(resnet18_pretrained, id_val_loader, loss_criterion, device)\n",
        "# print('Validation performance before finetuning -- loss: %10.8f, accuracy: %10.8f'%(val_loss, val_accuracy))\n",
        "\n",
        "# finetune the model\n",
        "train_losses = []\n",
        "train_accuracies = []\n",
        "val_losses = []\n",
        "val_accuracies = []\n",
        "id_val_losses = []\n",
        "id_val_accuracies = []\n",
        "\n",
        "for epoch in range(1, num_epochs+1):\n",
        "  train_loss, train_accuracy = train_epoch(densenet121_pretrained, train_loader, loss_criterion, optimizer, device)\n",
        "  val_loss, val_accuracy = eval_epoch(densenet121_pretrained, id_val_loader, loss_criterion, device)\n",
        "  train_losses.append(train_loss)\n",
        "  id_val_losses.append(val_loss)\n",
        "  train_accuracies.append(train_accuracy)\n",
        "  id_val_accuracies.append(val_accuracy)\n",
        "  print('\\nEpoch %d'%(epoch))\n",
        "  print('train loss: %10.8f, accuracy: %10.8f'%(train_loss, train_accuracy))\n",
        "  print('id_val loss: %10.8f, accuracy: %10.8f'%(val_loss, val_accuracy))\n"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nU5aLksBhofd",
        "outputId": "6d45aff0-0fbc-4d48-c8ca-0e37df75fca4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "360it [00:38,  9.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val loss: 0.88477289, accuracy: 85.02546297\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "run_eval_ood(densenet121_pretrained, val_loader, loss_criterion, device, \"val\")"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fKONpbBWhofe",
        "outputId": "108fc3e8-d840-459c-fe96-bb478e8864ef"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run_eval_ood(densenet121_pretrained, test_loader, loss_criterion, device, \"test\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BR2vwURB3FRI",
        "outputId": "00df7cde-cdcd-400b-def0-83c3c4dcada1"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "878it [01:33,  9.37it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test loss: 1.37542546, accuracy: 76.98248861\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "outputs": [],
      "source": [
        "torch.save(resnet18_pretrained.state_dict(), \"resnet18_pretrained.pt\")"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "LH3Ra_hnhofe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "outputs": [],
      "source": [
        "torch.save(densenet121_pretrained.state_dict(), \"densenet121_pretrained.pt\")"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "Kfu2kIaZhofe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "3119it [04:37, 11.22it/s]\n",
            "347it [00:30, 11.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1\n",
            "train loss: 0.47145916, accuracy: 87.70353364\n",
            "id_val loss: 0.40097229, accuracy: 88.23246879\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "3119it [03:44, 13.88it/s]\n",
            "347it [00:21, 16.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 2\n",
            "train loss: 0.44362384, accuracy: 88.75984748\n",
            "id_val loss: 0.55559740, accuracy: 88.59870317\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "3119it [03:26, 15.13it/s]\n",
            "347it [00:21, 15.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 3\n",
            "train loss: 0.47033815, accuracy: 88.75798676\n",
            "id_val loss: 0.39019356, accuracy: 92.20100865\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "3119it [03:26, 15.14it/s]\n",
            "347it [00:21, 16.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 4\n",
            "train loss: 0.44872888, accuracy: 89.02879242\n",
            "id_val loss: 0.33573079, accuracy: 91.41750720\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "3119it [03:25, 15.15it/s]\n",
            "347it [00:21, 15.99it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 5\n",
            "train loss: 0.46100884, accuracy: 88.96567123\n",
            "id_val loss: 0.58201889, accuracy: 89.65237752\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "resnetxt50_pretrained = models.resnext50_32x4d(pretrained=True)\n",
        "\n",
        "# freeze all the layers\n",
        "for param in resnetxt50_pretrained.parameters():\n",
        "  param.requires_grad = False\n",
        "\n",
        "resnetxt50_pretrained.fc = nn.Linear(in_features=2048, out_features=2, bias=True)\n",
        "\n",
        "resnetxt50_pretrained.to(device)\n",
        "\n",
        "# create a SGD optimizer\n",
        "optimizer = torch.optim.SGD(resnetxt50_pretrained.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "# set up loss function\n",
        "loss_criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "# finetune the model\n",
        "train_losses = []\n",
        "train_accuracies = []\n",
        "id_val_losses = []\n",
        "id_val_accuracies = []\n",
        "for epoch in range(1, num_epochs+1):\n",
        "  train_loss, train_accuracy = train_epoch(resnetxt50_pretrained, train_loader, loss_criterion, optimizer, device)\n",
        "  val_loss, val_accuracy = eval_epoch(resnetxt50_pretrained, id_val_loader, loss_criterion, device)\n",
        "  train_losses.append(train_loss)\n",
        "  id_val_losses.append(val_loss)\n",
        "  train_accuracies.append(train_accuracy)\n",
        "  id_val_accuracies.append(val_accuracy)\n",
        "  print('\\nEpoch %d'%(epoch))\n",
        "  print('train loss: %10.8f, accuracy: %10.8f'%(train_loss, train_accuracy))\n",
        "  print('id_val loss: %10.8f, accuracy: %10.8f'%(val_loss, val_accuracy))\n"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N-spuoIyhofe",
        "outputId": "1e9be0d2-a85c-4d64-f220-735bd6999213"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run_eval_ood(resnetxt50_pretrained, val_loader, loss_criterion, device, \"val\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WqrwywFa_AbA",
        "outputId": "8c9dfe0a-585d-4835-943a-adce35870665"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "360it [00:39,  9.01it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val loss: 1.31229609, accuracy: 78.37673611\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "run_eval_ood(resnetxt50_pretrained, test_loader, loss_criterion, device, \"test\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c9GAxnmI_BMv",
        "outputId": "90cc2c79-2f7a-425a-991e-a12f217843a1"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "878it [01:38,  8.91it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test loss: 3.62986293, accuracy: 55.57018793\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(densenet121_pretrained.state_dict(), \"resnetxt50_pretrained.pt\")"
      ],
      "metadata": {
        "id": "5zmE54z4_GfQ"
      },
      "execution_count": 73,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "name": "generalization_task_camelyon.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}